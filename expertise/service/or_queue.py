import os, openreview, shutil
from typing import *
from dataclasses import dataclass, field
from csv import reader

from .queue import JobData, JobQueue, TwoStepQueue

from ..execute_expertise import *

@dataclass
class DatasetInfo(JobData):
    """
    Keeps track of the create_dataset queue information and status. Dataset directory will be overwritten by the server.
    Same information as expertise info but requires an authenticated token
    """
    token: str = field(
        default='',
        metadata={"help": "The authenticated token of the user client"}
    )
    baseurl: str = field(
        default='',
        metadata={"help": "The base URL of the API to call to log in"}
    )

class ExpertiseQueue(JobQueue):
    """
    Keeps track of queue metadata and is responsible for queuing jobs when given a config for running the expertise model
    """
    def get_result(self, user_id: str, delete_on_get: bool = True, job_id: str = '', job_name: str = '') -> dict:
        """
        Return the result of the job submitted by user_id with either the given job_id or job_name
        If no job_id is provided, uses job_name
        if no job_name is provided, uses job_id
        By default, deletes the data and metadata with the associated job

        :param user_id: A string containing the user id that has submitted jobs
        :type user_id: str

        :param delete_on_get: A boolean flag that decides whether or not to maintain the data and metadata from the job
        :type delete_on_get: bool

        :param job_id: A string containing the submitted job id
        :type job_id: str

        :param job_name: A string containing the user specified name for the job
        :type job_name: str

        :rtype: dict
        """
        ret_list: List[dict] = []
        # Retrieve the single job data object
        self.logger.info('ExpertiseQueue: Retrieving results from an expertise job')
        matching_jobs: List[JobData] = self._get_job_data(user_id, job_id=job_id, job_name=job_name)
        assert len(matching_jobs) == 1, 'Too many matching jobs, search by job ID'
        current_job = matching_jobs[0]

        # Build the return list by reading the csv under the job_name.csv file
        cwd = os.getcwd()
        job_path = os.path.join(cwd, current_job.job_id)
        csv_path = os.path.join(current_job.config['model_params']['scores_path'], f"{current_job.config['name']}.csv")
        self.logger.info('ExpertiseQueue: Reading data from generated by the job')
        with open(csv_path, 'r') as csv_file:
            data_reader = reader(csv_file)
            for row in data_reader:
                ret_list.append({
                    'submission': row[0],
                    'user': row[1],
                    'score': float(row[2])
                })
        
        # Check flag and clear directory
        self.logger.info(f'ExpertiseQueue: Checking delete on get flag - {delete_on_get}')
        if delete_on_get == True:
            self.logger.info('ExpertiseQueue: Deleting directory')
            shutil.rmtree(job_path)
        
        self.logger.info(f'ExpertiseQueue: Returning results from job user {user_id} with either job ID [{job_id}] or job name [{job_name}]')
        return {'results': ret_list}
            
    def run_job(self, config: dict) -> None:
        """The actual work, set of functions to be run in a subprocess from the _handle_job thread"""
        execute_expertise(config_file=config)


class UserPaperQueue(TwoStepQueue):
    """
    Keeps track of queue metadata and is responsible for queuing jobs when given a config for getting the data for the expertise model
    """
    def _prepare_job(self, job_info: JobData) -> None:
        """
        Given a job object, modify its config to work with the server after its job ID is assigned

        :param job_info: A JobData object whose fields will be modified with respect to the job ID
        :type job_info: JobData
        """
        # Overwrite certain keys in the config
        filepath_keys = ['work_dir', 'scores_path', 'publications_path', 'submissions_path']
        file_keys = ['csv_expertise', 'csv_submissions']

        # Filter keys by membership in the config
        if 'model_params' not in job_info.config.keys():
            job_info.config['model_params'] = {}
        filepath_keys = [key for key in filepath_keys if key in job_info.config['model_params'].keys()]
        file_keys = [key for key in file_keys if key in job_info.config.keys()]

        # First handle dataset -> directory
        if 'dataset' not in job_info.config.keys():
            job_info.config['dataset'] = {}
            job_info.config['dataset']['directory'] = f"./{job_info.job_id}"
        else:
            if 'directory' not in job_info.config['dataset'].keys():
                job_info.config['dataset']['directory'] = f"./{job_info.job_id}"
            else:
                job_info.config['dataset']['directory'] = JobQueue._augment_path(job_info, job_info.config['dataset']['directory'])

        if not os.path.isdir(job_info.config['dataset']['directory']):
            os.makedirs(job_info.config['dataset']['directory'])

        # Next handle other file paths
        for key in filepath_keys:
            job_info.config['model_params'][key] = JobQueue._augment_path(job_info, job_info.config['model_params'][key])
        
        # Now, write data stored in the file keys to disk
        for key in file_keys:
            output_file = key + '.csv'
            write_to_dir = os.path.join(job_info.config['dataset']['directory'], output_file)

            # Add newline characters, write to file and set the field in the config to the directory of the file
            for idx, data in enumerate(job_info.config[key]):
                job_info.config[key][idx] = data.strip() + '\n'
            with open(write_to_dir, 'w') as csv_out:
                csv_out.writelines(job_info.config[key])
            
            job_info.config[key] = output_file
        
        # Set SPECTER+MFR paths
        job_info.config['model_params']['specter_dir'] = '../expertise-utils/specter/'
        job_info.config['model_params']['mfr_feature_vocab_file'] = '../expertise-utils/multifacet_recommender/feature_vocab_file'
        job_info.config['model_params']['mfr_checkpoint_dir'] = '../expertise-utils/multifacet_recommender/mfr_model_checkpoint/'

    def put_job(self, request: DatasetInfo) -> None:
        """
        Adds a DatasetInfo object to the queue to be processed asynchronously
        Augments the request's config with the authenticated token
        
        :param request: A DatasetInfo object containing the metadata of the job to be executed
        :type request: DatasetInfo
        """
        # Update the config with the token and base URL
        self.logger.info('DatasetQueue: augmenting the request config with credentials...')
        request.config.update({
            'token': request.token,
            'baseurl': request.baseurl
        })
        self.logger.info('DatasetQueue: enqueuing the request')
        super().put_job(request)

    def run_job(self, config: dict) -> None:
        """The actual work, set of functions to be run in a subprocess from the _handle_job thread"""
        openreview_client = openreview.Client(
            token=config['token'],
            baseurl=config['baseurl']
        )
        execute_create_dataset(openreview_client, config_file=config)