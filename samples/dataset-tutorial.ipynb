{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `expertise.dataset` module tutorial\n",
    "\n",
    "Install from https://github.com/iesl/openreview-expertise\n",
    "\n",
    "`pip install -e <openreview-expertise directory location>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from expertise import dataset\n",
    "\n",
    "dataset_dir = '/Users/michaelspector/projects/openreview/openreview-datasets/datasets/akbc19_dblp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dataset object is initialized with a valid dataset directory. \n",
    "A dataset directory has the following structure:\n",
    "\n",
    "/example_dataset\n",
    "    /submissions\n",
    "        <paper1_id.jsonl>\n",
    "        <paper2_id.jsonl>\n",
    "        ...\n",
    "    /archives\n",
    "        <reviewer1_id.jsonl>\n",
    "        <reviewer2_id.jsonl>\n",
    "        ...\n",
    "    /bids\n",
    "        <paper1_id.jsonl>\n",
    "        <paper2_id.jsonl>\n",
    "    /extras\n",
    "        # conference-specific files can go here\n",
    "    metadata.json\n",
    "\n",
    "'''\n",
    "\n",
    "dataset = dataset.Dataset(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reviewer_count': 67,\n",
       " 'submission_count': 54,\n",
       " 'archive_counts': {'~Aishwarya_Kamath1': {'arx': 4, 'bid': 52},\n",
       "  '~Alan_Ritter1': {'arx': 43, 'bid': 9},\n",
       "  '~Alexander_Spangher1': {'arx': 0, 'bid': 35},\n",
       "  '~Amrita_Saha1': {'arx': 17, 'bid': 10},\n",
       "  '~Anca_Dumitrache1': {'arx': 15, 'bid': 10},\n",
       "  '~Andreas_Vlachos1': {'arx': 67, 'bid': 54},\n",
       "  '~Andrew_McCallum1': {'arx': 148, 'bid': 2},\n",
       "  '~Anna_Lisa_Gentile1': {'arx': 29, 'bid': 7},\n",
       "  '~Ashwin_Ittoo1': {'arx': 1, 'bid': 2},\n",
       "  '~Bhuwan_Dhingra1': {'arx': 16, 'bid': 18},\n",
       "  '~Bishan_Yang1': {'arx': 14, 'bid': 0},\n",
       "  '~Caiming_Xiong1': {'arx': 115, 'bid': 18},\n",
       "  '~Camilo_John_Thorne1': {'arx': 21, 'bid': 0},\n",
       "  '~Chen_Liang1': {'arx': 63, 'bid': 54},\n",
       "  '~Chris_Welty1': {'arx': 15, 'bid': 12},\n",
       "  '~Danqi_Chen1': {'arx': 18, 'bid': 0},\n",
       "  '~Dirk_Weissenborn1': {'arx': 30, 'bid': 53},\n",
       "  '~Emma_Strubell1': {'arx': 27, 'bid': 6},\n",
       "  '~Federica_Cena1': {'arx': 37, 'bid': 7},\n",
       "  '~Gabriel_Stanovsky1': {'arx': 10, 'bid': 76},\n",
       "  '~Gerhard_Weikum1': {'arx': 42, 'bid': 16},\n",
       "  '~Guillaume_Bouchard2': {'arx': 37, 'bid': 0},\n",
       "  '~Heike_Adel1': {'arx': 14, 'bid': 54},\n",
       "  '~Hoifung_Poon1': {'arx': 54, 'bid': 0},\n",
       "  '~Ivan_Titov1': {'arx': 78, 'bid': 54},\n",
       "  '~James_Ferguson1': {'arx': 3, 'bid': 0},\n",
       "  '~Jeff_Mitchell1': {'arx': 6, 'bid': 54},\n",
       "  '~Jie_Yang1': {'arx': 12, 'bid': 13},\n",
       "  '~Jieyu_Zhao1': {'arx': 18, 'bid': 30},\n",
       "  '~Jin-Dong_Kim1': {'arx': 63, 'bid': 0},\n",
       "  '~Johannes_Welbl2': {'arx': 12, 'bid': 54},\n",
       "  '~John_Philip_McCrae1': {'arx': 1, 'bid': 54},\n",
       "  '~Kate_Silverstein1': {'arx': 3, 'bid': 43},\n",
       "  '~Kristian_Kersting1': {'arx': 159, 'bid': 12},\n",
       "  '~Laura_Dietz1': {'arx': 24, 'bid': 54},\n",
       "  '~Li_Dong2': {'arx': 0, 'bid': 3},\n",
       "  '~Mandar_Joshi1': {'arx': 2, 'bid': 0},\n",
       "  '~Mark_Yatskar1': {'arx': 11, 'bid': 10},\n",
       "  '~Mayank_Kejriwal1': {'arx': 18, 'bid': 32},\n",
       "  '~Michael_J._Witbrock1': {'arx': 40, 'bid': 16},\n",
       "  '~Muhao_Chen1': {'arx': 13, 'bid': 54},\n",
       "  '~Nitish_Gupta1': {'arx': 11, 'bid': 5},\n",
       "  '~Omer_Levy1': {'arx': 53, 'bid': 54},\n",
       "  '~Panupong_Pasupat1': {'arx': 11, 'bid': 54},\n",
       "  '~Partha_Talukdar1': {'arx': 20, 'bid': 4},\n",
       "  '~Pasquale_Minervini1': {'arx': 18, 'bid': 10},\n",
       "  '~Pat_Verga1': {'arx': 24, 'bid': 1},\n",
       "  '~Paul_Buitelaar1': {'arx': 85, 'bid': 54},\n",
       "  '~Pedro_Szekely1': {'arx': 28, 'bid': 0},\n",
       "  '~Philipp_Cimiano1': {'arx': 94, 'bid': 39},\n",
       "  '~Rainer_Gemulla1': {'arx': 27, 'bid': 52},\n",
       "  '~Rajarshi_Das1': {'arx': 46, 'bid': 37},\n",
       "  '~Robin_Jia1': {'arx': 9, 'bid': 54},\n",
       "  '~Sebastian_Krause1': {'arx': 25, 'bid': 54},\n",
       "  '~Sebastian_Pado1': {'arx': 78, 'bid': 54},\n",
       "  '~Sewon_Min1': {'arx': 9, 'bid': 11},\n",
       "  '~Simon_Razniewski1': {'arx': 5, 'bid': 54},\n",
       "  '~Svitlana_Vakulenko1': {'arx': 13, 'bid': 2},\n",
       "  '~Svitlana_Volkova1': {'arx': 42, 'bid': 0},\n",
       "  '~Timothee_Lacroix1': {'arx': 1, 'bid': 54},\n",
       "  '~Wen-tau_Yih1': {'arx': 48, 'bid': 54},\n",
       "  '~Xiang_Li2': {'arx': 5, 'bid': 54},\n",
       "  '~Xiang_Zhao1': {'arx': 56, 'bid': 54},\n",
       "  '~Xiao_Yu1': {'arx': 60, 'bid': 15},\n",
       "  '~Yi_Luan1': {'arx': 6, 'bid': 0},\n",
       "  '~Yu_Su2': {'arx': 42, 'bid': 14},\n",
       "  '~Zhilin_Yang1': {'arx': 41, 'bid': 0}},\n",
       " 'bid_counts': {'B1gjSZ9TpQ': 34,\n",
       "  'BJerQWcp6Q': 31,\n",
       "  'BJgrxbqp67': 35,\n",
       "  'BJlh0x9ppQ': 36,\n",
       "  'BJlsVZ966m': 33,\n",
       "  'BkeCW-q6aQ': 27,\n",
       "  'BketCg9p6X': 35,\n",
       "  'BygfyW5pam': 38,\n",
       "  'BylEpe9ppX': 36,\n",
       "  'H1gwRx5T6Q': 25,\n",
       "  'H1l8ag9a6m': 36,\n",
       "  'H1xdMZ566m': 39,\n",
       "  'H1xtpgqTaQ': 31,\n",
       "  'HJl_NZ5T67': 30,\n",
       "  'HJxYZ-5paX': 37,\n",
       "  'HJxeGb5pTm': 28,\n",
       "  'HJxikb5T6m': 30,\n",
       "  'HJxmxbq66Q': 37,\n",
       "  'Hkg5zW96p7': 29,\n",
       "  'Hkg7rbcp67': 32,\n",
       "  'HkgAyb5aaQ': 33,\n",
       "  'Hklbplqp6X': 33,\n",
       "  'HkyI-5667': 34,\n",
       "  'Hye97Zcaam': 34,\n",
       "  'HygfXWqTpm': 33,\n",
       "  'HylHE-9p6m': 30,\n",
       "  'HylaEWcTT7': 35,\n",
       "  'HylqxZqppX': 30,\n",
       "  'HylzClqp67': 29,\n",
       "  'S1eg7-9pp7': 31,\n",
       "  'S1xf-W5paX': 33,\n",
       "  'S1xlgbcT6X': 31,\n",
       "  'SJeHzZqapm': 32,\n",
       "  'SJxfV-q6Tm': 32,\n",
       "  'SklKr-qaT7': 30,\n",
       "  'SkxE1b56TQ': 35,\n",
       "  'Sye0lZqp6Q': 28,\n",
       "  'Sye7fZcTTm': 36,\n",
       "  'SyeD-b9T6m': 28,\n",
       "  'SygLHbcapm': 33,\n",
       "  'SylC6gcTTm': 31,\n",
       "  'SylxCx5pTQ': 32,\n",
       "  'r1e3WW5aTX': 39,\n",
       "  'r1gyNZ9Tam': 36,\n",
       "  'r1loaec6pm': 33,\n",
       "  'r1xP1W56pQ': 32,\n",
       "  'r1xTXZ9p6Q': 30,\n",
       "  'rJgHCgc6pX': 31,\n",
       "  'rJx2g-qaTm': 35,\n",
       "  'rkeFyWcTTm': 32,\n",
       "  'rklHbZqaTQ': 37,\n",
       "  'rygDeZqap7': 34,\n",
       "  'rylPm-5a67': 35,\n",
       "  'ryleB-56pQ': 26}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The dataset object exposes various statistics about itself:\n",
    "'''\n",
    "\n",
    "dataset.get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('~Svitlana_Volkova1', {'title': 'Online Bayesian Models for Personal Analytics in Social Media', 'abstract': 'Latent author attribute prediction in social media provides a novel set of conditions for the construction of supervised classification models. With individual authors as training and test instances, their associated content (\"features\") are made available incrementally over time, as they converse over discussion forums. We propose various approaches to handling this dynamic data, from traditional batch training and testing, to incremental bootstrapping, and then active learning via crowdsourcing. Our underlying model relies on an intuitive application of Bayes rule, which should be easy to adopt by the community, thus allowing for a general shift towards online modeling for social media.'})\n",
      "\n",
      "('~Svitlana_Volkova1', [{'title': 'Online Bayesian Models for Personal Analytics in Social Media', 'abstract': 'Latent author attribute prediction in social media provides a novel set of conditions for the construction of supervised classification models. With individual authors as training and test instances, their associated content (\"features\") are made available incrementally over time, as they converse over discussion forums. We propose various approaches to handling this dynamic data, from traditional batch training and testing, to incremental bootstrapping, and then active learning via crowdsourcing. Our underlying model relies on an intuitive application of Bayes rule, which should be easy to adopt by the community, thus allowing for a general shift towards online modeling for social media.'}, {'title': 'Inferring Perceived Demographics from User Emotional Tone and User-Environment Emotional Contrast', 'abstract': 'We examine communications in a social network to study user emotional contrast – the propensity of users to express different emotions than those expressed by their neighbors. Our analysis is based on a large Twitter dataset, consisting of the tweets of 123,513 users from the USA and Canada. Focusing on Ekman’s basic emotions, we analyze differences between the emotional tone expressed by these users and their neighbors of different types, and correlate these differences with perceived user demographics. We demonstrate that many perceived demographic traits correlate with the emotional contrast between users and their neighbors. Unlike other approaches on inferring user attributes that rely solely on user communications, we explore the network structure and show that it is possible to accurately predict a range of perceived demographic traits based solely on the emotions emanating from users and their neighbors.'}, {'title': 'Discourse, Health and Well-Being of Military Populations Through the Social Media Lens', 'abstract': 'Social media can provide a resource for characterizing communities and small populations through activities and content shared online. For instance, studying the language use in social media within military populations may provide insights into their health and well-being. In this paper, we address three research questions: (1) How do military populations use social media? (2) What do military users discuss in social media? And (3) Do military users talk about health and well-being differently than civilians? Military Twitter users were identified through keywords in the profile description of users who posted geo-tagged tweets at military installations. The data was anonymized for the analysis. User tweets that belong to military populations were compared to non-military populations. Our results indicate that military users talk more about events in their military life, whereas non-military users talk more about school, work, and leisure activities. Additionally, we identified significant differences in communication behavior between two populations, including health-related'}, {'title': 'Inferring Latent User Properties from Texts Published in Social Media', 'abstract': \"We demonstrate an approach to predict latent personal attributes including user demographics, online personality, emotions and sentiments from texts published on Twitter. We rely on machine learning and natural language processing techniques to learn models from user communications. We first examine individual tweets to detect emotions and opinions emanating from them, and then analyze all the tweets published by a user to infer latent traits of that individual. We consider various user properties including age, gender, income, education, relationship status, optimism and life satisfaction. We focus on Ekman's six emotions: anger, joy, surprise, fear, disgust and sadness. Our work can help social network users to understand how others may perceive them based on how they communicate in social media, in addition to its evident applications in online sales and marketing, targeted advertising, large scale polling and healthcare analytics.\"}, {'title': 'Social Media Predictive Analytics', 'abstract': 'The recent explosion of social media services like Twitter, Google+ and Facebook has led to an interest in social media predictive analytics – automatically inferring hidden information from the large amounts of freely available content. It has a number of applications, including: online targeted advertising, personalized marketing, large-scale passive polling and real-time live polling, personalized recommendation systems and search, and real-time healthcare analytics etc. In this tutorial, we will describe how to build a variety of social media predictive analytics for inferring latent user properties from a Twitter network including demographic traits, personality, interests, emotions and opinions etc. Our methods will address several important aspects of social media such as: dynamic, streaming nature of the data, multi-relationality in social networks, data collection and annotation biases, data and model sharing, generalization of the existing models, data drift, and scalability to other languages. We will start with an overview of the existing approaches for social media predictive analytics. We will describe the state-of-the-art static (batch) models and features. We will then present models for streaming (online) inference from single and multiple data streams; and formulate a latent attribute prediction task as a sequence-labeling problem. Finally, we present several techniques for dynamic (iterative) learning and prediction using active learning setup with rationale annotation and filtering. The tutorial will conclude with a practice session focusing on walk-through examples for predicting latent user properties e.g., political preferences, income, education level, life satisfaction and emotions emanating from user communications on Twitter.'}, {'title': 'Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources', 'abstract': 'In the age of social news, it is important to understand the types of reactions that are evoked from news sources with various levels of credibility. In the present work we seek to better understand how users react to trusted and deceptive news sources across two popular, and very different, social media platforms. To that end, (1) we develop a model to classify user reactions into one of nine types, such as answer, elaboration, and question, etc, and (2) we measure the speed and the type of reaction for trusted and deceptive news sources for 10.8M Twitter posts and 6.2M Reddit comments. We show that there are significant differences in the speed and the type of reactions between trusted and deceptive news sources on Twitter, but far smaller differences on Reddit.'}, {'title': 'Lightly Supervised Learning of Procedural Dialog Systems', 'abstract': 'Procedural dialog systems can help users achieve a wide range of goals. However, such systems are challenging to build, currently requiring manual engineering of substantial domain-specific task knowledge and dialog management strategies. In this paper, we demonstrate that it is possible to learn procedural dialog systems given only light supervision, of the type that can be provided by non-experts. We consider domains where the required task knowledge exists in textual form (e.g., instructional web pages) and where system builders have access to statements of user intent (e.g., search query logs or dialog interactions). To learn from such textual resources, we describe a novel approach that first automatically extracts task knowledge from instructions, then learns a dialog manager over this task knowledge to provide assistance. Evaluation in a Microsoft Office domain shows that the individual components are highly accurate and can be integrated into a dialog system that provides effective help to users.'}, {'title': 'Exploring Sentiment in Social Media: Bootstrapping Subjectivity Clues from Multilingual Twitter Streams', 'abstract': 'We study subjective language in social media and create Twitter-specific lexicons via bootstrapping sentiment-bearing terms from multilingual Twitter streams. Starting with a domain-independent, highprecision sentiment lexicon and a large pool of unlabeled data, we bootstrap Twitter-specific sentiment lexicons, using a small amount of labeled data to guide the process. Our experiments on English, Spanish and Russian show that the resulting lexicons are effective for sentiment classification for many underexplored languages in social media.'}, {'title': 'Inferring User Political Preferences from Streaming Communications', 'abstract': 'Existing models for social media personal analytics assume access to thousands of messages per user, even though most users author content only sporadically over time. Given this sparsity, we: (i) leverage content from the local neighborhood of a user; (ii) evaluate batch models as a function of size and the amount of messages in various types of neighborhoods; and (iii) estimate the amount of time and tweets required for a dynamic model to predict user preferences. We show that even when limited or no selfauthored data is available, language from friend, retweet and user mention communications provide sufficient evidence for prediction. When updating models over time based on Twitter, we find that political preference can be often be predicted using roughly 100 tweets, depending on the context of user selection, where this could mean hours, or weeks, based on the author’s tweeting frequency.'}, {'title': 'Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media', 'abstract': 'Different demographics, e.g., gender or age, can demonstrate substantial variation in their language use, particularly in informal contexts such as social media. In this paper we focus on learning gender differences in the use of subjective language in English, Spanish, and Russian Twitter data, and explore cross-cultural differences in emoticon and hashtag use for male and female users. We show that gender differences in subjective language can effectively be used to improve sentiment analysis, and in particular, polarity classification for Spanish and Russian. Our results show statistically significant relative F-measure improvement over the gender-independent baseline 1.5% and 1% for Russian, 2% and 0.5% for Spanish, and 2.5% and 5% for English for polarity and subjectivity classification.'}, {'title': 'Learning to Relate Literal and Sentimental Descriptions of Visual Properties', 'abstract': 'Language can describe our visual world at many levels, including not only what is literally there but also the sentiment that it invokes. In this paper, we study visual language, both literal and sentimental, that describes the overall appearance and style of virtual characters. Sentimental properties, including labels such as “youthful” or “country western,” must be inferred from descriptions of the more literal properties, such as facial features and clothing selection. We present a new dataset, collected to describe Xbox avatars, as well as models for learning the relationships between these avatars and their literal and sentimental descriptions. In a series of experiments, we demonstrate that such learned models can be used for a range of tasks, including predicting sentimental words and using them to rank and build avatars. Together, these results demonstrate that sentimental language provides a concise (though noisy) means of specifying low-level visual properties.'}, {'title': 'Multilingual Connotation Frames: A Case Study on Social Media for Targeted Sentiment Analysis and Forecast'}, {'title': 'ESTEEM: A Novel Framework for Qualitatively Evaluating and Visualizing Spatiotemporal Embeddings in Social Media'}, {'title': 'Interactive Machine Learning at Scale With CHISSL'}, {'title': 'Separating Facts from Fiction: Linguistic Models to Classify Suspicious and Trusted News Posts on Twitter'}, {'title': 'Predicting Foreign Language Usage from English-Only Social Media Posts'}, {'title': 'Truth of Varying Shades: Analyzing Language in Fake News and Political Fact-Checking'}, {'title': 'Contrasting Public Opinion Dynamics and Emotional Response During Crisis', 'abstract': 'We propose an approach for contrasting spatiotemporal dynamics of public opinions expressed toward targeted entities, also known as stance detection task, in Russia and Ukraine during crisis. Our analysis relies on a novel corpus constructed from posts on the VKontakte social network, centered on local public opinion of the ongoing RussianUkrainian crisis, along with newly annotated resources for predicting expressions of fine-grained emotions including joy, sadness, disgust, anger, surprise and fear. Akin to prior work on sentiment analysis we align traditional public opinion polls with aggregated automatic predictions of sentiments for contrastive geo-locations. We report interesting observations on emotional response and stance variations across geo-locations. Some of our findings contradict stereotypical misconceptions imposed by media, for example, we found posts from Ukraine that do not support Euromaidan but support Putin, and posts from Russia that are against Putin but in favor USA. Furthermore, we are the first to demonstrate contrastive stance variations over time across geo-locations using storyline visualization technique.'}, {'title': 'Collection : Crawling Users , Neighbors and Their Communication for Personal Attribute Prediction in Social Media', 'abstract': ''}, {'title': 'Vulnerable to Misinformation? Verifi!', 'abstract': 'We present Verifi2, a visual analytic system to support the investigation of misinformation on social media. On the one hand, social media platforms empower individuals and organizations by democratizing the sharing of information. On the other hand, even well-informed and experienced social media users are vulnerable to misinformation. To address the issue, various models and studies have emerged from multiple disciplines to detect and understand the effects of misinformation. However, there is still a lack of intuitive and accessible tools that help social media users distinguish misinformation from verified news. In this paper, we present Verifi2, a visual analytic system that uses state-of-the-art computational methods to highlight salient features from text, social network, and images. By exploring news on a source level through multiple coordinated views in Verifi2, users can interact with the complex dimensions that characterize misinformation and contrast how real and suspicious news outlets differ on these dimensions. To evaluate Verifi2, we conduct interviews with experts in digital media, journalism, education, psychology, and computing who study misinformation. Our interviews show promising potential for Verifi2 to serve as an educational tool on misinformation. Furthermore, our interview results highlight the complexity of the problem of combating misinformation and call for more work from the visualization community.'}, {'title': 'Cost-Effective Personal Analytics in Social Media Using Language and Network Structure', 'abstract': 'Limitations to public access of social media, such as increased rate throttling under the revised Twitter API, prompts rethinking current approaches for a variety of inference tasks in social networks, such as the prediction of latent author attributes. We investigate various novel network construction methods over Twitter users in order to leverage information from different interpretations of a “local” neighborhood, aimed at a cost-effective model for personal analytics. We evaluate our model as a function of the size of the neighborhood, along with the amount of content data (tweets) associated with those neighbors. We show that even when limited or no selfauthored data is available, content from the local neighborhood can provide sufficient evidence for prediction. We demonstrate that data from friends, retweets and user mentions is a stronger signal in this regard than their replies and shared hashtags.'}, {'title': 'Animal Disease Event Recognition and Classification', 'abstract': 'Monitoring epidemic crises, caused by rapid spread of infectious animal diseases, can be facilitated by the plethora of information about disease-related events that is available online. Therefore, the ability to use this information to perform domain-specific entity recognition and event-related sentence classification, which in turn can support time and space visualization of automatically extracted events, is highly desirable. Towards this goal, we present a rule-based approach to the problem of extracting animal disease-related events from web documents. Our approach relies on the recognition of structured entity tuples, consisting of attributes, which describe events related to animal diseases. The event attributes that we consider include animal diseases, dates, species and geo-referenced locations. We perform disease names and species recognition using an automatically-constructed ontology, dates are extracted using regular expressions, while location are extracted using a conditional random fields tool. The extracted events are further classified as confirmed or suspected based on semantic features, obtained from the e.g., GoogleSets and WordNet. Our preliminary results demonstrate the feasibility of the proposed approach.'}, {'title': 'Predicting Foreign Language Usage from English-Only Social Media Posts', 'abstract': 'Social media is known for its multi-cultural and multilingual interactions, a natural product of which is code-mixing. Multilingual speakers mix languages they tweet to address a different audience, express certain feelings, or attract attention. This paper presents a largescale analysis of 6 million tweets produced by 27 thousand multilingual users speaking 12 other languages besides English. We rely on this corpus to build predictive models to infer non-English languages that users speak exclusively from their English tweets. Unlike native language identification task, we rely on large amounts of informal social media communications rather than ESL essays. We contrast the predictive power of the state-of-theart machine learning models trained on lexical, syntactic, and stylistic signals with neural network models learned from word, character and byte representations extracted from English only tweets. We report that content, style and syntax are the most predictive of nonEnglish languages that users speak on Twitter. Neural network models learned from byte representations of user content combined with transfer learning yield the best performance. Finally, by analyzing cross-lingual transfer – the influence of non-English languages on various levels of linguistic performance in English, we present novel findings on stylistic and syntactic variations across speakers of 12 languages in social media.'}, {'title': 'Contrasting Public Opinion Dynamics and Emotional Response During Crisis', 'abstract': 'We propose an approach for contrasting spatiotemporal dynamics of public opinions expressed toward targeted entities, also known as stance detection task, in Russia and Ukraine during crisis. Our analysis relies on a novel corpus constructed from posts on the VKontakte social network, centered on local public opinion of the ongoing RussianUkrainian crisis, along with newly annotated resources for predicting expressions of fine-grained emotions including joy, sadness, disgust, anger, surprise and fear. Akin to prior work on sentiment analysis we align traditional public opinion polls with aggregated automatic predictions of sentiments for contrastive geo-locations. We report interesting observations on emotional response and stance variations across geo-locations. Some of our findings contradict stereotypical misconceptions imposed by media, for example, we found posts from Ukraine that do not support Euromaidan but support Putin, and posts from Russia that are against Putin but in favor USA. Furthermore, we are the first to demonstrate contrastive stance variations over time across geo-locations using storyline visualization technique.'}, {'title': 'Truth of Varying Shades: Analyzing Language in Fake News and Political Fact-Checking', 'abstract': 'We present an analytic study on the language of news media in the context of political fact-checking and fake news detection. We compare the language of real news with that of satire, hoaxes, and propaganda to find linguistic characteristics of untrustworthy text. To probe the feasibility of automatic political fact-checking, we also present a case study based on PolitiFact.com using their factuality judgments on a 6-point scale. Experiments show that while media fact-checking remains to be an open research question, stylistic cues can help determine the truthfulness of text.'}, {'title': 'Can You Verifi This? Studying Uncertainty and Decision-Making About Misinformation Using Visual Analytics', 'abstract': 'We describe a novel study of decision-making processes around misinformation on social media. Using a custom-built visual analytic system, we presented users with news content from social media accounts from a variety of news outlets, including outlets engaged in distributing misinformation. We conducted controlled experiments to study decision-making regarding the veracity of these news outlets and tested the role of confirmation bias (the tendency to ignore contradicting information) and uncertainty of information on human decision-making processes. Our findings reveal that the presence of conflicting information, presented to users in the form of cues, impacts the ability to judge the veracity of news in systematic ways. We also find that even instructing participants to explicitly disconfirm given hypotheses does not significantly impact their decision-making regarding misinformation when compared to a control condition. Our findings have the potential to inform the design of visual analytics systems so that they may be used to mitigate the effects of cognitive biases and stymie the spread of misinformation on social media.'}, {'title': 'Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources', 'abstract': 'In the age of social news, it is important to understand the types of reactions that are evoked from news sources with various levels of credibility. In the present work we seek to better understand how users react to trusted and deceptive news sources across two popular, and very different, social media platforms. To that end, (1) we develop a model to classify user reactions into one of nine types, such as answer, elaboration, and question, etc, and (2) we measure the speed and the type of reaction for trusted and deceptive news sources for 10.8M Twitter posts and 6.2M Reddit comments. We show that there are significant differences in the speed and the type of reactions between trusted and deceptive news sources on Twitter, but far smaller differences on Reddit.'}, {'title': 'Multilingual Connotation Frames: A Case Study on Social Media for Targeted Sentiment Analysis and Forecast', 'abstract': 'People around the globe respond to major real world events through social media. To study targeted public sentiments across many languages and geographic locations, we introduce multilingual connotation frames: an extension from English connotation frames of Rashkin et al. (2016) with 10 additional European languages, focusing on the implied sentiments among event participants engaged in a frame. As a case study, we present large scale analysis on targeted public sentiments toward salient events and entities using 1.2 million multilingual connotation frames extracted from Twitter.'}, {'title': 'Interactive Machine Learning at Scale With CHISSL', 'abstract': 'We demonstrate CHISSL a scalable client-server system for real-time interactive machine learning. Our system is capable of incorporating user feedback incrementally and immediately without a pre-defined prediction task. Computation is partitioned between a lightweight web-client and a heavyweight server. The server relies on representation learning and off-the-shelf agglomerative clustering to find a dendrogram, which we use to quickly approximate distances in the representation space. The client, using only this dendrogram, incorporates user feedback via transduction. Distances and predictions for each unlabeled instance are updated incrementally and deterministically, with O(n) space and time complexity. Our algorithm is implemented in a functional prototype, designed to be easy to use by non-experts. The prototype organizes the large amounts of data into recommendations. This allows the user to interact with actual instances by dragging and dropping to provide feedback in an intuitive manner. We applied CHISSL to several domains including cyber, social media, and geo-temporal analysis. Motivation and Background Often analysts are handed huge piles of data such as news articles, images, or social media posts that need to be summarized, organized, or triaged. Machine learning can help with this task, but only when quality labeled training data is available. Such data can be difficult to obtain – labeling cannot be crowdsourced, for example, if expertise is limited to a few individuals. Experienced data scientists know that realworld systems require a human-in-the-loop to provide labels for instances and to correct prediction errors (Amershi, Fogarty, and Weld 2012). However, current human-in-the-loop systems have scalability limitations for both machine and human components, e.g., they are slow to incorporate user feedback and overwhelm the user by showing all the data as points in abstract statistical visualizations. Our contributions in this paper are performance and design improvements for CHISSL (Arendt, Komurlu, and Blaha 2017), a framework for Computer-Human Interaction for Semi-Supervised Learning. These contributions allow CHISSL to overcome the aforementioned scalability issues. The primary task we support with CHISSL is to perform Copyright c © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. transductive learning by allowing the user to explore and organize large amounts of unlabeled instances into groups that need not be defined a priori. In future work, we plan to allow the user to export their grouping as a training set for a classification model to facilitate inductive learning. There are several approaches for human-in-the-loop transductive learning including spatialization, interactive clustering, and active learning. Spatialization is a visual analytics technique where the user’s mental model of a dataset is learned and projected into a two dimensional space. Brown et al. 2012 pioneered variants of this approach where model parameters were learned directly from user interactions e.g., dragging an instance on the screen. A disadvantage of these techniques is that they require the user to interpret groups from a 2-D projection of the data, and such projections can be misleading (Chuang et al. 2012). Interactive clustering addresses a similar problem by allowing a user to guide a clustering algorithm with rejection (Srivastava, Zou, and Sutton 2016) or providing linkage constraints (Bilenko, Basu, and Mooney 2004). Likewise, supervised and semisupervised learning has been implemented in systems that elicit user feedback and perform label induction or transduction to update predicted labels (Kulesza et al. 2014). Furthermore, instances are usually represented abstractly in statistical plots, making it unclear about what instances may be in need of correction. Finally, the above techniques are not scalable because they require too much time to incorporate user feedback or they show all instances on the screen, overwhelming the user.'}, {'title': 'Social Media Predictive Analytics', 'abstract': 'The recent explosion of social media services like Twitter, Google+ and Facebook has led to an interest in social media predictive analytics – automatically inferring hidden information from the large amounts of freely available content. It has a number of applications, including: online targeted advertising, personalized marketing, large-scale passive polling and real-time live polling, personalized recommendation systems and search, and real-time healthcare analytics etc. In this tutorial, we will describe how to build a variety of social media predictive analytics for inferring latent user properties from a Twitter network including demographic traits, personality, interests, emotions and opinions etc. Our methods will address several important aspects of social media such as: dynamic, streaming nature of the data, multi-relationality in social networks, data collection and annotation biases, data and model sharing, generalization of the existing models, data drift, and scalability to other languages. We will start with an overview of the existing approaches for social media predictive analytics. We will describe the state-of-the-art static (batch) models and features. We will then present models for streaming (online) inference from single and multiple data streams; and formulate a latent attribute prediction task as a sequence-labeling problem. Finally, we present several techniques for dynamic (iterative) learning and prediction using active learning setup with rationale annotation and filtering. The tutorial will conclude with a practice session focusing on walk-through examples for predicting latent user properties e.g., political preferences, income, education level, life satisfaction and emotions emanating from user communications on Twitter.'}, {'title': 'Identifying Effective Signals to Predict Deleted and Suspended Accounts on Twitter Across Languages', 'abstract': 'Social networks have an ephemerality to them where accounts and messages are constantly being edited, deleted, or marked as private. This continuous change comes from concerns around privacy, a potential desire for to be forgotten and suspicious behavior. In this study we present a novel task – predicting suspicious e.g., to be deleted or suspended accounts in social media. We analyze multiple datasets of thousands of active, deleted and suspended Twitter accounts to produce a series of predictive representations for the removal or shutdown of an account. We selected these accounts from speakers of three languages – Russian, Spanish, and English to evaluate if speakers of various languages behave differently with regards to deleting accounts. We compared the predictive power of the state-of-the-art machine learning models to recurrent neutral networks trained on previously unexplored features. Furthermore, this work is the first to rely on image and affect signals in addition to language and network to predict deleted and suspended accounts in social media. We found that unlike widely used profile and network features, the discourse of deleted or suspended versus active accounts forms the basis for highly accurate account deletion and suspension prediction. More precisely, we observed that the presence of certain terms in tweets leads to a higher likelihood for that user’s account deletion or suspension. Moreover, despite image and affect signals yield lower predictive performance compared to language, they reveal interesting behavioral differences across speakers of different languages. Our extensive analysis and novel findings on language use and suspicious behavior of speakers of different languages can improve the existing approaches to credibility analysis, disinformation and deception detection in social media.'}, {'title': 'Fishing for Clickbaits in Social Images and Texts with Linguistically-Infused Neural Network Models', 'abstract': 'This paper presents the results and conclusions of our participation in the Clickbait Challenge 2017 on automatic clickbait detection in social media. We first describe linguistically-infused neural network models and identify informative representations to predict the level of clickbaiting present in Twitter posts. Our models allow to answer the question not only whether a post is a clickbait or not, but to what extent it is a clickbait post e.g., not at all, slightly, considerably, or heavily clickbaity using a score ranging from 0 to 1. We evaluate the predictive power of models trained on varied text and image representations extracted from tweets. Our best performing model that relies on the tweet text and linguistic markers of biased language extracted from the tweet and the corresponding page yields mean squared error (MSE) of 0.04, mean absolute error (MAE) of 0.16 and R2 of 0.43 on the held-out test data. For the binary classification setup (clickbait vs. non-clickbait), our model achieved F1 score of 0.69. We have not found that image representations combined with text yield significant performance improvement yet. Nevertheless, this work is the first to present preliminary analysis of objects extracted using Google Tensorflow object detection API from images in clickbait vs. non-clickbait Twitter posts. Finally, we outline several steps to improve model performance as a part of the future work.'}, {'title': 'On Predicting Sociodemographic Traits and Emotions from Communications in Social Networks and Their Implications to Online Self-Disclosure', 'abstract': \"Social media services such as Twitter and Facebook are virtual environments where people express their thoughts, emotions, and opinions and where they reveal themselves to their peers. We analyze a sample of 123,000 Twitter users and 25 million of their tweets to investigate the relation between the opinions and emotions that users express and their predicted psychodemographic traits. We show that the emotions that we express on online social networks reveal deep insights about ourselves. Our methodology is based on building machine learning models for inferring coarse-grained emotions and psychodemographic profiles from user-generated content. We examine several user attributes, including gender, income, political views, age, education, optimism, and life satisfaction. We correlate these predicted demographics with the emotional profiles emanating from user tweets, as captured by Ekman's emotion classification. We find that some users tend to express significantly more joy and significantly less sadness in their tweets, such as those predicted to be in a relationship, with children, or with a higher than average annual income or educational level. Users predicted to be women tend to be more opinionated, whereas those predicted to be men tend to be more neutral. Finally, users predicted to be younger and liberal tend to project more negative opinions and emotions. We discuss the implications of our findings to online privacy concerns and self-disclosure behavior.\"}, {'title': 'Using Social Media to Predict the Future: A Systematic Literature Review', 'abstract': 'Social media (SM) data provides a vast record of humanity’s everyday thoughts, feelings, and actions at a resolution previously unimaginable. Because user behavior on SM is a reflection of events in the real world, researchers have realized they can use SM in order to forecast, making predictions about the future. The advantage of SM data is its relative ease of acquisition, large quantity, and ability to capture socially relevant information, which may be difficult to gather from other data sources. Promising results exist across a wide variety of domains, but one will find little consensus regarding best practices in either methodology or evaluation. In this systematic review, we examine relevant literature over the past decade, tabulate mixed results across a number of scientific disciplines, and identify common pitfalls and best practices. We find that SM forecasting is limited by data biases, noisy data, lack of generalizable results, a lack of domain-specific theory, and underlying complexity in many prediction tasks. But despite these shortcomings, recurring findings and promising results continue to galvanize researchers and demand continued investigation. Based on the existing literature, we identify research practices which lead to success, citing specific examples in each case and making recommendations for best practices. These recommendations will help researchers take advantage of the exciting possibilities offered by SM platforms.'}, {'title': 'Predicting User Demographics , Emotions and Opinions in Social Networks', 'abstract': \"Social networks are virtual environments where people express their thoughts, emotions, and opinions. We analyze a large sample of 123,000 Twitter users and 25 million tweets to investigate the relation between user emotions and predicted demographics. Our methodology is based on building machine learning models to predict emotions and demographic profiles from user content. We report novel demographic-affect correlations and its implications on online self-disclosure. AUDIENCE [Natural Language Processing] [Machine Learning] [Social Network Analysis] [Opinion Mining] [Emotion Detection] [Demographic Classification] [Advanced Technical Talk] INTRODUCTION Twitter and Facebook are prominent social networks, used regularly by over 1/7th of the world's population. Researchers used the massive volumes of data to study how users present themselves and the language they use [1], showing how to predict user psycho-demographic profiles [2,3], user emotions [4], and well-being. This study analyzes user communications in a social network on a large scale— 25 million tweets, 123,513 user profiles—examining a range of automatically detected emotions, opinions, and a variety of demographic traits. This work can help social network users to understand how others may perceive them based on how they communicate in social media, in addition to its evident applications in online sales and marketing, targeted advertising, large-scale polling, and healthcare analytics. DATA Focusing on Twitter, we use crowdsourcing to get demographic labels for a sample of U = 5,000 users (Table 1), and train machine learning classifiers to predict these demographic traits from the textual content generated by these users. We then apply attribute classifiers to get labels for a much larger sample of U = 123,513 users. We use a similar method for labeling emotions expressed in user text. We train an emotion classifier on an initial sample of TE = 52,925 tweets, then use the classifier to get emotion labels for a much larger sample of T = 24,919,528 tweets. However, rather than obtaining emotion tags for the initial sample through crowdsourcing, we use tweets annotated with emotional hashtags such as #disgust or #anger, identifying a specific emotion. To perform a reliable analysis of the differences in emotion expressed by different user groups, our demographic and emotion predictions must be highly accurate. We report that our models for emotion and demographic classification outperform the existing state-of-the-art systems. Our high-level methodology is shown in Figure 1. Figure 1. Our approach for predicting user demographics, emotions, and opinions in social media. Attribute Binary Attribute Values Gender Male: 2,124, Female: 2,874 Age Below 25: 2,511, Above 25: 1,372 Ethnicity Afr. Amer.: 1,705, Caucasian: 2,409 Education High school: 3,423, Degree: 1,575 Income <$35K: 3,324, >$35K: 1,675 Children Yes: 797, No: 4203 Optimism Pessimist: 907, Optimist: 2,655 Life Satisfaction Dissatisfied: 840, Satisfied: 2,949 Table 1. Profile annotations collected via crowdsourcing.\"}, {'title': 'How Humans versus Bots React to Deceptive and Trusted News Sources: A Case Study of Active Users', 'abstract': 'Society’s reliance on social media as a primary source of news has spawned a renewed focus on the spread of misinformation. In this work, we identify the differences in how social media accounts identified as bots react to news sources of varying credibility, regardless of the veracity of the content those sources have shared. We analyze bot and human responses annotated using a fine-grained model that labels responses as being an answer, appreciation, agreement, disagreement, an elaboration, humor, or a negative reaction. We present key findings of our analysis into the prevalence of bots, the variety and speed of bot and human reactions, and the disparity in authorship of reaction tweets between these two sub-populations. We observe that bots are responsible for 9-15% of the reactions to sources of any given type but comprise only 7-10% of accounts responsible for reaction-tweets; trusted news sources have the highest proportion of humans who reacted; bots respond with significantly shorter delays than humans when posting answerreactions in response to sources identified as propaganda. Finally, we report significantly different inequality levels in reaction rates for accounts identified as bots vs not.'}, {'title': 'RuSentiment: An Enriched Sentiment Analysis Dataset for Social Media in Russian', 'abstract': 'This paper presents RuSentiment, a new dataset for sentiment analysis of social media posts in Russian, and a new set of comprehensive annotation guidelines that are extensible to other languages. RuSentiment is currently the largest in its class for Russian, with 31,185 posts annotated with Fleiss’ kappa of 0.58 (3 annotations per post). To diversify the dataset, 6,950 posts were pre-selected with an active learning-style strategy. We report baseline classification results, and we also release the best-performing word embeddings trained on 3.2B corpus of Russian social media posts.'}, {'title': 'ESTEEM: A Novel Framework for Qualitatively Evaluating and Visualizing Spatiotemporal Embeddings in Social Media', 'abstract': 'Analyzing and visualizing large amounts of social media communications and contrasting short-term conversation changes over time and geolocations is extremely important for commercial and government applications. Earlier approaches for largescale text stream summarization used dynamic topic models and trending words. Instead, we rely on text embeddings – low-dimensional word representations in a continuous vector space where similar words are embedded nearby each other. This paper presents ESTEEM,1 a novel tool for visualizing and evaluating spatiotemporal embeddings learned from streaming social media texts. Our tool allows users to monitor and analyze query words and their closest neighbors with an interactive interface. We used stateof-the-art techniques to learn embeddings and developed a visualization to represent dynamically changing relations between words in social media over time and other dimensions. This is the first interactive visualization of streaming text representations learned from social media texts that also allows users to contrast differences across multiple dimensions of the data.'}, {'title': 'Using Social Media to Measure Student Wellbeing: A Large-Scale Study of Emotional Response in Academic Discourse', 'abstract': 'Student resilience and emotional wellbeing are essential for both academic and social development. Earlier studies on tracking students’ happiness in academia showed that many of them struggle with mental health issues. For example, a 2015 study at the University of California Berkeley found that 47% of graduate students suffer from depression, following a 2005 study that showed 10% had considered suicide. This is the first large-scale study that uses signals from social media to evaluate students’ emotional wellbeing in academia. This work presents fine-grained emotion and opinion analysis of 79,329 tweets produced by students from 44 universities. The goal of this study is to qualitatively evaluate and compare emotions and sentiments emanating from students’ communications across different academic discourse types and across universities in the U.S. We first build novel predictive models to categorize academic discourse types generated by students into personal, social, and general categories. We then apply emotion and sentiment classification models to annotate each tweet with six Ekman’s emotions – joy, fear, sadness, disgust, anger, and surprise and three opinion types – positive, negative, and neutral. We found that emotions and opinions expressed by students vary across discourse types and universities, and correlate with survey-based data on student satisfaction, happiness and stress. Moreover, our results provide novel insights on how students use social media to share academic information, emotions, and opinions that would pertain to students academic performance and emotional well-being.'}, {'title': 'Measuring, Predicting and Visualizing Short-Term Change in Word Representation and Usage in VKontakte Social Network', 'abstract': 'Language in social media is extremely dynamic: new words emerge, trend and disappear, while the meaning of existing words can fluctuate over time. This work addresses several important tasks of visualizing and predicting short term text representation shift, i.e. the change in a word’s contextual semantics. We study the relationship between short-term concept drift and representation shift on a large social media corpus – VKontakte collected during the Russia-Ukraine crisis in 2014 – 2015. We visualize short-term representation shift for example keywords and build predictive models to forecast short-term shifts in meaning from previous meaning as well as from concept drift. We show that short-term representation shift can be accurately predicted up to several weeks in advance and that visualization provides insight into meaning change. Our approach can be used to explore and characterize specific aspects of the streaming corpus during crisis events and potentially improve other downstream classification tasks including real-time event forecasting in social media.'}, {'title': 'Intrinsic and Extrinsic Evaluation of Spatiotemporal Text Representations in Twitter Streams', 'abstract': 'Language in social media is a dynamic system, constantly evolving and adapting, with words and concepts rapidly emerging, disappearing, and changing their meaning. These changes can be estimated using word representations in context, over time and across locations. A number of methods have been proposed to track these spatiotemporal changes but no general method exists to evaluate the quality of these representations. Previous work largely focused on qualitative evaluation, which we improve by proposing a set of visualizations that highlight changes in text representation over both space and time. We demonstrate usefulness of novel spatiotemporal representations to explore and characterize specific aspects of the corpus of tweets collected from European countries over a two-week period centered around the terrorist attacks in Brussels in March 2016. In addition, we quantitatively evaluate spatiotemporal representations by feeding them into a downstream classification task – event type prediction. Thus, our work is the first to provide both intrinsic (qualitative) and extrinsic (quantitative) evaluation of text representations for spatiotemporal trends.'}, {'title': 'Separating Facts from Fiction: Linguistic Models to Classify Suspicious and Trusted News Posts on Twitter', 'abstract': 'Pew research polls report 62 percent of U.S. adults get news on social media (Gottfried and Shearer, 2016). In a December poll, 64 percent of U.S. adults said that “made-up news” has caused a “great deal of confusion” about the facts of current events (Barthel et al., 2016). Fabricated stories in social media, ranging from deliberate propaganda to hoaxes and satire, contributes to this confusion in addition to having serious effects on global stability. In this work we build predictive models to classify 130 thousand news posts as suspicious or verified, and predict four subtypes of suspicious news – satire, hoaxes, clickbait and propaganda. We show that neural network models trained on tweet content and social network interactions outperform lexical models. Unlike previous work on deception detection, we find that adding syntax and grammar features to our models does not improve performance. Incorporating linguistic features improves classification results, however, social interaction features are most informative for finer-grained separation between four types of suspicious news posts.'}])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A Dataset object's `archives`, `submissions`, and `bids` functions return generators \n",
    "that stream tuples of records in the format (id: str, records: list or dict)\n",
    "'''\n",
    "\n",
    "archives_as_dicts = dataset.archives(\n",
    "    fields=[\n",
    "        'title',\n",
    "        'abstract',\n",
    "        'fulltext',\n",
    "        'keywords',\n",
    "        'subject_areas'\n",
    "    ],\n",
    "    return_batches=False #default\n",
    ")\n",
    "\n",
    "print(next(archives_as_dicts))\n",
    "print()\n",
    "\n",
    "archives_as_lists = dataset.archives(\n",
    "    fields=[\n",
    "        'title',\n",
    "        'abstract',\n",
    "        'fulltext',\n",
    "        'keywords',\n",
    "        'subject_areas'\n",
    "    ],\n",
    "    return_batches=True\n",
    ")\n",
    "\n",
    "print(next(archives_as_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "every other submission record:  96%|█████████▋| 26/27.0 [00:00<00:00, 623.40it/s]\n",
      "every 3rd reviewer's archive: 23it [00:00, 175.64it/s]                                        \n",
      "every 4th bid, keyed on submission id: 14it [00:00, 55.00it/s]                          \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "There are various convenience options (e.g. partition_id and num_partitions, which can be useful for parallelization)\n",
    "'''\n",
    "\n",
    "submission_generator = dataset.submissions(\n",
    "    fields=['title'],\n",
    "    return_batches=True,\n",
    "    progressbar=\"every other submission record\",\n",
    "    partition_id=0,\n",
    "    num_partitions=2\n",
    ")\n",
    "every_other_submission = [submission_id for submission_id, submission in submission_generator]\n",
    "\n",
    "archive_generator = dataset.archives(\n",
    "    fields=['title'],\n",
    "    return_batches=True,\n",
    "    progressbar=\"every 3rd reviewer's archive\",\n",
    "    partition_id=0,\n",
    "    num_partitions=3\n",
    ")\n",
    "every_third_reviewer = [reviewer_id for reviewer_id, archive in archive_generator]\n",
    "\n",
    "bid_generator = dataset.bids(\n",
    "    return_batches=True,\n",
    "    progressbar=\"every 4th bid, keyed on submission id\",\n",
    "    partition_id=0,\n",
    "    num_partitions=4\n",
    ")\n",
    "every_fourth_paper_bidcount = [(submission_id, len(bids)) for submission_id, bids in bid_generator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openreview",
   "language": "python",
   "name": "openreview"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
