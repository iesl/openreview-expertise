{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "\n",
    "How well do TPMS scores predict bids?\n",
    "\n",
    "The deliverable will be a plot showing the performance of the TPMS scores on the \"recall@M\" evaluation metric.\n",
    "\n",
    "Plan:\n",
    "- load eval data for UAI 18 conference\n",
    "- load a model (which could be pre-trained, if the model requires training)\n",
    "- evaluate the model and produce graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import json, csv\n",
    "import itertools\n",
    "\n",
    "import openreview\n",
    "import numpy\n",
    "\n",
    "from collections import defaultdict\n",
    "from expertise.models import tfidf, randomize, model_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "\n",
    "Load up data and organize it into various indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/uai18/'\n",
    "\n",
    "submission_records_dir = os.path.join(data_dir, 'submission_records_fulltext')\n",
    "reviewer_archives_dir = os.path.join(data_dir, 'reviewer_archives')\n",
    "reviewer_bids_file = os.path.join(data_dir, 'reviewer_bids.jsonl')\n",
    "tfidf_scores_file = os.path.join(data_dir, 'tfidf-score-matrix.txt')\n",
    "tpms_scores_file = os.path.join(data_dir,'tpms-score-matrix.txt')\n",
    "random_scores_file = os.path.join(data_dir, 'random-score-matrix.txt')\n",
    "\n",
    "papers = []\n",
    "for filename in os.listdir(submission_records_dir):\n",
    "    with open(os.path.join(submission_records_dir, filename)) as f:\n",
    "        papers.append(openreview.Note(**json.load(f)))\n",
    "        \n",
    "paper_by_number = {n.number: n for n in papers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_ids = []\n",
    "for filename in os.listdir(reviewer_archives_dir):\n",
    "    reviewer_ids.append(filename.replace('.jsonl',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bids = []\n",
    "\n",
    "with open(reviewer_bids_file) as f:\n",
    "    for line in f.readlines():\n",
    "        all_bids.append(openreview.Tag.from_json(json.loads(line.replace('\\n',''))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_bids = {\n",
    "    'i want to review': [],\n",
    "    'i can review': [],\n",
    "    'i can probably review but am not an expert': [],\n",
    "    'i cannot review': [],\n",
    "    'no bid': []\n",
    "}\n",
    "\n",
    "users_w_bids = set()\n",
    "for bid in all_bids:\n",
    "    binned_bids[bid.tag.lower()].append(bid)\n",
    "    users_w_bids.update(bid.signatures)\n",
    "\n",
    "for b in binned_bids:\n",
    "    print(b, len(binned_bids[b]))\n",
    "\n",
    "bids_by_forum = defaultdict(list)\n",
    "for bid in all_bids:\n",
    "    bids_by_forum[bid.forum].append(bid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_scores(file):\n",
    "    score_matrix = {}\n",
    "    with open(file) as f:\n",
    "        lines = [line.replace('\\n','') for line in f.readlines()]\n",
    "\n",
    "    for line in lines:\n",
    "        note_id, reviewer_id, score = eval(line)\n",
    "        if note_id not in score_matrix:\n",
    "            score_matrix[note_id] = {}\n",
    "        if reviewer_id not in score_matrix[note_id]:\n",
    "            score_matrix[note_id][reviewer_id] = score\n",
    "        else:\n",
    "            raise('pair already seen ', note_id, reviewer_id, score)\n",
    "\n",
    "    ranklists = model_utils.matrix_to_ranklists(score_matrix)\n",
    "    return ranklists\n",
    "\n",
    "\n",
    "tfidf_ranklists = read_scores(tfidf_scores_file)\n",
    "tpms_ranklists = read_scores(tpms_scores_file)\n",
    "random_ranklists = read_scores(random_scores_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluation & Graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from expertise.evaluators import recall_at_m\n",
    "from expertise import graphing\n",
    "import numpy as np\n",
    "import matplotlib, matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = recall_at_m.Evaluator(bids_by_forum=bids_by_forum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_avg_scores(ranklists, evaluator):\n",
    "    all_scores = []\n",
    "    for forum, scores in evaluator.evaluate(ranklists):\n",
    "        all_scores.append(scores)\n",
    "    if all_scores:\n",
    "        avg_scores = np.mean(all_scores, axis=0)\n",
    "    else:\n",
    "        avg_scores = []\n",
    "        \n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tpms_avg_scores = get_avg_scores(tpms_ranklists, evaluator)\n",
    "tfidf_avg_scores = get_avg_scores(tfidf_ranklists, evaluator)\n",
    "random_avg_scores = get_avg_scores(random_ranklists, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax = graphing.recall_at_m(tpms_avg_scores, ax, 'tpms')\n",
    "ax = graphing.recall_at_m(tfidf_avg_scores, ax, 'tfidf')\n",
    "ax = graphing.recall_at_m(random_avg_scores, ax, 'random')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# filename = 'tpms-tfidf-random-filtered_ranklist'\n",
    "# fig.savefig(\"./{0}.png\".format(filename), dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openreview",
   "language": "python",
   "name": "openreview"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
